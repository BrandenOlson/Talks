\documentclass[mathserif,compress,xcolor={dvipsnames}]{beamer}

\mode<presentation>
{
  \usecolortheme{orchid}
  \useoutertheme{shadow}
}
\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{natbib, verbatim}

\usepackage[utf8]{inputenc}

\usepackage{mathpazo}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{blkarray}
\usepackage{mathptmx}
\usepackage{anyfontsize}
\usepackage{t1enc}
\usepackage{appendix}
\usepackage{array}
\usepackage{bm}
\usepackage{cancel}
\usepackage{cite}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{empheq}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{units}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{algorithm, algorithmic}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{}


\usepackage{color}
\lstset{language=R,basicstyle=\ttfamily,breaklines=true,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{magenta}\ttfamily,
                showstringspaces=false,
                }

\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newcommand*\mb{\mathbf}
\newcommand*\reals{\mathbb{R}}
\newcommand*\complex{\mathbb{C}}
\newcommand*\naturals{\mathbb{N}}
\newcommand*\nats{\naturals}
\newcommand*\integers{\mathbb{Z}}
\newcommand*\rationals{\mathbb{Q}}
\newcommand*\irrationals{\mathbb{J}}
\newcommand*\pd{\partial}
\newcommand*\htab{\hspace{4 mm}}
\newcommand*\vtab{\vspace{0.5 in}}
\newcommand*\lsent{\mathcal{L}}
\newcommand*\conj{\overline}
\newcommand*\union{\cup}
\newcommand*\intersect{\cap}
\newcommand*\cl{\cancel}
\newcommand*\ANS{\text{ANS}}
\newcommand*\As{\text{As}}
\newcommand*\then{\rightarrow}
\newcommand*\elim{\text{E}}
\newcommand*\intro{\text{I}}
\newcommand*\absurd{\curlywedge}
\newcommand*\NK{\vdash_{\text{NK}}}
\newcommand*\derivation{\begin{tabular} { >{$}l<{$}  >{$}c<{$}  >{$}l<{$}  >{$}r<{$} }}
\newcommand*\interp{\mathcal{I}}
\newcommand*\ba{\[ \begin{aligned}}
\newcommand*\ea{\end{aligned} \]}
\newcommand*\C{\mathcal{C}}
\newcommand*\D{\mathscr{D}}
\newcommand*\e{\operatorname{e}}
\newcommand*\df{=_{\text{def}}}
\newcommand*\eps{\epsilon}
\newcommand*\enum{\begin{enumerate}[label=(\alph*)]}
\newcommand*\enumend{\end{enumerate}}
\newcommand*\E[1]{\mathsf{E}\left[#1\right]}
\newcommand*\Esub[2]{\mathsf{E}_{#1}\left[#2\right]}
\newcommand*\Var[1]{\mathsf{Var}\left[#1\right]}
\newcommand*\Cov[1]{\mathsf{Cov}\left[#1\right]}
\newcommand*\iid{\overset{\text{iid}}{\sim}}
\newcommand*\Exp[1][\lambda]{\text{Exp}(\text{rate}=#1)}
\newcommand*\ind[2]{I_{({#1}, {#2})} }
\newcommand*\set[1]{\left\{#1\right\}}
\newcommand*\estim[1]{\widehat{#1}}
\newcommand*\der{\text{d}}
\newcommand*\norm[1]{\left\|#1\right\|}
\newcommand*\dist[2]{\;\text{dist}\left(#1, #2\right)}
\newcommand*\interior{\text{int}\;}
\newcommand*\exterior{\text{ext}\;}
\newcommand*\boundary{\text{bd}\;}
\newcommand*\lh{\overset{\text{L'H}}{=}}

\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*{\argmin}{arg\;min}
\renewcommand\;{\,}
\renewcommand\epsilon{\varepsilon}
\renewcommand\rho{\varrho}
\renewcommand\phi{\varphi}
\renewcommand\mod{\hspace{0.2em} \textbf{mod}\hspace{0.2em}}
\renewcommand\Pr[1]{ \mathsf{Pr}\left(#1\right)}
\def\ci{\perp\!\!\!\perp}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}
\usepackage{adjustbox}

\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]

\lstset{breaklines=true,
        numbersep=5pt,
        xleftmargin=.25in,
        xrightmargin=.25in}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}


\newcommand{\real}{\ensuremath{\mathbb{R}}}
\newcommand{\bA}{\mbox{\protect\boldmath $A$}}
\newcommand{\bo}{\mbox{\protect\boldmath $o$}}
\newcommand{\bu}{\mbox{\protect\boldmath $u$}}
\newcommand{\by}{\mbox{\protect\boldmath $y$}}
\newcommand{\bx}{\mbox{\protect\boldmath $x$}}
\newcommand{\bs}{\mbox{\protect\boldmath $s$}}
\newcommand{\bS}{\mbox{\protect\boldmath $S$}}
\newcommand{\bz}{\mbox{\protect\boldmath $z$}}
\newcommand{\bh}{\mbox{\protect\boldmath $h$}}
\newcommand{\bF}{\mbox{\protect\boldmath $f$}}
\newcommand{\bt}{\mbox{\protect\boldmath $t$}}
\newcommand{\bc}{\mbox{\protect\boldmath $c$}}
\newcommand{\bC}{\mbox{\protect\boldmath $C$}}
\newcommand{\bV}{\mbox{\protect\boldmath $V$}}
\newcommand{\bX}{\mbox{\protect\boldmath $X$}}
\newcommand{\bW}{\mbox{\protect\boldmath $W$}}
\newcommand{\bZ}{\mbox{\protect\boldmath $Z$}}
\newcommand{\bof}{\mbox{\protect\boldmath $f$}}
\newcommand{\indicator}{{\ensuremath{\mathbb{I}}}}
\newcommand{\M}{{\ensuremath{\rm M}}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\hsp}{\hspace{0.2mm}}

\footnotesize

\beamertemplatenavigationsymbolsempty
\setbeamertemplate{headline}{\vskip2pt}

\title[]{Statistical methods for adaptive immune receptor repertoire analysis and comparison}

\author[]
{Branden Olson}

\date[Feb. 28, 2020]
{February 28, 2020}

\institute[]
{
Department of Statistics
\\
University of Washington Seattle
}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection]
   \end{frame}
}

\expandafter\def\expandafter\insertshorttitle\expandafter{%
  \insertshorttitle\hspace{22em}%
  \insertframenumber}

\begin{document}

\begin{frame}[noframenumbering]
  \titlepage
\end{frame}

\section{Introduction to T cell receptors}

\begin{frame}\frametitle{TCRs, cont.}
\begin{center}
\includegraphics[width=0.6\linewidth]{Images/TCR.png}
\end{center}
\end{frame}

\begin{frame}\frametitle{BCRs and TCRs}
\begin{center}
\includegraphics[width=0.5\linewidth]{Images/bcr_tcr.png}
\end{center}
\end{frame}


\begin{frame}\frametitle{The formation of BCRs (TCRs are simpler)}
\begin{center}
\includegraphics[width=\linewidth]{Images/BCRFormation.png}
\end{center}
\end{frame}

\begin{frame}\frametitle{Adaptive immune receptor repertoires}
\begin{itemize}
\item[]
A TCR \emph{repertoire} is the collection of TCRs in an individual at a given moment
\bigskip
\item[]
This changes in time
\bigskip
\item[]
Can sample via high-throughput sequencing
\medskip
\begin{itemize}
\item
We call these samples AIRR-seq (\textbf{A}daptive \textbf{I}mmune \textbf{R}eceptor \textbf{R}epertoire \textbf{seq}uence) datasets
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{TCR repertoire comparison}
\begin{itemize}
\bigskip
\item
Often, immunologists wish to compare distributions of repertoires
\bigskip
\item
But comparing (and even visualizing) TCR distributions not quite straightforward...
\end{itemize}
\begin{center}
\includegraphics[width=0.8\linewidth]{Images/TCRRepertoire.png}
\end{center}
\end{frame}

\begin{frame}\frametitle{TCR distributions}
\begin{center}
\includegraphics[width=0.6\linewidth]{Images/transport-cartoon.png}
\end{center}
\end{frame}

\begin{frame}\frametitle{Why is this difficult?}
\begin{itemize}
\item
Comparing discrete distributions is easy! \\ ...but requires dense sampling
\bigskip
\item
Typical divergences (e.g. $\ell_1$) can't account for similarities between genes
\bigskip
\item
Global vs. local differences?
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Let's use distances}
\begin{itemize}
\item
Idea: equip TCRs with a similarity metric
\bigskip
\item
Use distances to inform distribution comparisons
\bigskip
\item
Optimal transport is a class of methods to do this
\bigskip
\begin{itemize}
\item
Not the only such class (discrepancy/Prokhorov metrics) ...
\bigskip
\item
... but has some nice properties and intuitive interpretations
\end{itemize}
\end{itemize}
\end{frame}

\section{Introduction to optimal transport}



\begin{frame}\frametitle{Problem: Dispatching soldiers}
\begin{center}
\includegraphics[width=0.8\linewidth]{Images/soldiers.png}
\end{center}
{\scriptsize * Figure credit: Drs. Marco Cuturi and Justin Solomon}
\end{frame}

\begin{frame}\frametitle{Problem: Dispatching soldiers}
\begin{center}
\includegraphics[width=0.8\linewidth]{Images/soldiers_as_OT.png}
\end{center}
{\scriptsize * Figure credit: Drs. Marco Cuturi and Justin Solomon}
\end{frame}

\begin{frame}\frametitle{Problem: Moving dirt}
How can we fill up the hole using the smallest amount of "effort"?
\bigskip
\includegraphics[width=\linewidth]{Images/EMD_shovel.png}
{\scriptsize * Photo credit: Dr. Marco Cuturi}
\end{frame}

\begin{frame}\frametitle{Optimal transport}
\begin{itemize}
\item
The field of optimal transport solves these types of problems
\bigskip
\item
Just need a {\color{Green} distance} ($\equiv$ cost) function 
${\color{Green} d}:\mathcal{X} \times \mathcal{Y} \to \reals^+$ for {\color{Purple} moving} objects around
\bigskip
\item
Optimizations of the form
\begin{align}
	\ & \min_{{\color{violet} \pi} \in \text{couplings}({\color{red} \mu}, {\color{blue} \nu})} 
		\int_{\mathcal X \times \mathcal Y} 
		{\color{Green} d}(x, y) \der {\color{violet} \pi}(x, y) \\
	\equiv & \min_{{\color{violet} \pi} \in 
		\text{couplings}({\color{red} \mu}, {\color{blue} \nu})}
		 \Esub{(X, Y) \sim {\color{violet} \pi}}{ {\color{Green} d}(X, Y) }
\end{align}
\smallskip
\item
Modern techniques approximate this solution very fast (like, linearly!)
\end{itemize}
\end{frame}


\begin{frame}\frametitle{TCR distances}
\begin{itemize}
\item
Need a distance between TCRs to apply these methods
\bigskip
\item
Very much a research question
\bigskip
\item
Dash et al. (2017) introduce a TCR distance based on amino acid differences between TCR subregions
\end{itemize}
\end{frame}


\begin{frame}
\begin{center}
\includegraphics[width=0.7\linewidth]{Images/TCRdist.png}
\end{center}
\end{frame}

\begin{frame}\frametitle{What we're doing}
\begin{itemize}
\item
Goal: identify ``lonely'' TCRs with respect to another repertoire
\medskip
\begin{itemize}
\item
I.e., TCRs that are typical of their own repertoire but not of another
\end{itemize}
\bigskip
\item
Score via total ``effort'' within a TCRdist-ball of a given TCR:
\medskip
\ba
\text{Effort}(t_1, t_2) & := \pi_\text{optimal}(t_1, t_2) \text{TCRdist}(t_1, t_2) \\
\text{Loneliness}(t \in R_2 | R_1) & := \sum_{n \in \text{neighbors}(t)} \sum_{t' \in R_1}  \text{Effort}(t', n) 
\ea
\item
Validate these scores using sets of repertoires from similar or different cell types
\end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}\frametitle{Discussion}
\begin{center}
\includegraphics[width=0.35\linewidth]{Images/tm_bound.png}
\end{center}
\vspace{2em}
\begin{center}
\includegraphics[width=0.3\linewidth]{Images/sumrep_logo.png}
\hspace{6em}
\includegraphics[width=0.25\linewidth]{Images/transport-cartoon.png}
\end{center}
\end{frame}

\begin{frame}\frametitle{Acknowledgments}
\begin{minipage}{0.49\linewidth}
\begin{center}
Erick Matsen
\\
\includegraphics[width=0.5\linewidth]{Images/Erick.jpeg}
\end{center}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\begin{center}
Phil Bradley
\\
\includegraphics[width=0.4\linewidth]{Images/Phil.jpg}
\end{center}
\end{minipage}
\end{frame}

\begin{frame}{References}
\small
\begin{itemize}
\item[]
Cuturi M (2013). Sinkhorn distances: lightspeed computation of optimal transportation distances. {\em Advances in Neural Information Processing Systems}, \textbf{26}:2292--2300. arXiv:1306.0895
\bigskip
\item[]
Dale G et al. (2019). Clustered Mutations at the Murine and Human IgH Locus Exhibit Significant Linkage Consistent with Templated Mutagenesis. {\it The Journal of Immunology}, \textbf{ ji1801615}.
\bigskip
\item[]
Dash P, Fiore-Gartland AJ, Hertz T, et al. (2017). Quantifiable predictive features define epitope-specific T cell receptor repertoires. {\em Nature}, \textbf{547}(7661):89--93. doi:10.1038/nature22383
\bigskip
\item[]
Yeap et al. (2015). Sequence-Intrinsic Mechanisms that Target AID Mutational Outcomes on Antibody Genes. {\em Cell}, \textbf{163}(5):1124-1137. 
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
\Huge
Questions?
\end{center}
\end{frame}

\begin{frame}
\begin{center}
\includegraphics[width=0.5\linewidth]{Images/shield.png}
\end{center}
\end{frame}


\begin{frame}\frametitle{TCRdist stuff}
\begin{itemize}
\item
BLOSUM62: widely-used substitution (similarity) matrix for amino acids that was estimated using log odds scoring of frequencies from a large and trusted alignment database (called BLOCKS).
\bigskip
\item
CDR2.5: a loop between CDR2 and CDR3 (IMGT positions 81-86) that has been observed to make
contacts with pMHC in solved structures
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Problem: Moving dirt}
Perhaps we identify a function $T$ that takes the dirt at each spot $x$ and moves it to new spot $y = T(x)$
\bigskip
\includegraphics[width=\linewidth]{Images/EMD_shovel.png}
\end{frame}

\begin{frame}\frametitle{Problem: Moving dirt}
This defines a relationship between measures $\mu$ and $\nu$
\bigskip
\includegraphics[width=\linewidth]{Images/EMD_pushforward.png}
\end{frame}

\begin{frame}\frametitle{Pushforward mesaure}
\begin{itemize}
\item
Let $\mathbf T : \mathcal X \to \mathcal Y$ be a continuous map from sample space $\mathcal X$ to sample space $\mathcal Y$.
\item
The pushforward operator $\mathbf T_\sharp : \mathcal M(\mathcal X) \to \mathcal M(\mathcal Y)$, which maps measures on $\mathcal X$ to measures on $\mathcal Y$, satisfies for a given measure $\mu \in \mathcal M(\mathcal X)$,
 $\forall B \subset \mathcal Y$, $B$ measurable, we have
\begin{align}
    \left(\mathbf T_\sharp \mu\right) (B) 
    & = \mu\left( \{ \mathbf x \in \mathcal X : \mathbf T(\mathcal X) \in B \} \right) \\
    & = \mu\left(\mathbf T^{-1}(B) \right)
\end{align}

In the discrete case, which will be the case for TCR repertoires, we have
\ba
    \mathbf T_\sharp \mu := \sum_{i=1}^n a_i \delta_{\mathbf T(\mathbf x_i)}
\ea
for some weights $a_1, \dotsc, a_n$. 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{}
\begin{itemize}
\item
Let $\mu(\mathbf x) D(\mathbf x, \mathbf T(\mathbf x))$ be the work to move the mass at $\mathbf x$ to $\mathbf T(\mathbf x)$
\item
We seek the $\mathbf T$ which minimizes the total work over our domain $\mathcal X$:
\ba
\estim{\mathbf T }
	& = \argmin_{\mathbf T_\sharp\mu = \nu} \int_{\mathcal X} D(\mathbf x, \mathbf T(\mathbf x)) \;\der \mu(\mathbf x)
\ea
\item
This is known as Monge's problem
\item
This is a strict formulation and cannot be applied to most practical problems
\item
But, it's easy to picture and sets up the intuition for more general problems
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Justification for entropic regularization}
Basic rationale:
\begin{align*}
\; & \text{Entropic constraint} \\
& \implies \text{principile of maximum entropy} \\
& \implies \text{most smooth joint probability given a cost level} \\
& \implies \text{more robust notion of distance}
\end{align*}
For a given pair $({\color{red} \mathbf r}, {\color{blue} \mathbf c})$, finding plausible transportation plans with low cost (where plausibility
is measured by entropy) is more informative than finding extreme plans that are
extremely unlikely to appear in nature
\end{frame}

\begin{frame}\frametitle{Visualizing entropic regularization}
\begin{center}
\includegraphics[width=\linewidth]{Images/EntropyPlots.png}
\end{center}
* Here, $\gamma := \frac{1}{\lambda}$
\end{frame}

\begin{frame}\frametitle{Extreme sparsity}
\begin{tikzpicture}
        \node [draw, circle, thick, ProcessBlue, minimum size=2cm, align=center] at   (0,0)   {Experimental\\sample\\($10^2 - 10^6$)};
        \node [draw, ellipse, thick, orange, minimum width=5cm, minimum height=4cm, align=center] at   (1,0) {};
        \node [minimum width=7cm, orange, minimum height=5cm, xshift=3mm, align=center] at   (2,0) {Individual\\repertoire\\($10^{12}$)};
        \node [draw, ellipse, thick, LimeGreen, minimum width=7.7cm, minimum height=5cm, align=center] at (2.25, 0) {};
        \node [minimum width=7cm, LimeGreen, minimum height=5cm, xshift=2.9cm, align=center] at (1.8, 0) {Population\\repertoire\\($10^{12} - 10^{21}$)};
         \node [draw, ellipse, thick, Fuchsia, minimum width=11cm, minimum height=6cm, align=center] at (3.8, 0) {};
         \node [Fuchsia, minimum width=11cm, minimum height=6cm, align=center, xshift=3.9cm] at (3.8, 0) {All possible \\protein sequences \\($20^{100}$)};
    \end{tikzpicture}


\end{frame}

\begin{frame}\frametitle{Example: discrete distribution}
\begin{itemize}
\item[]
Let $\mathcal X = \{{\color{orange} 1}, {\color{orange} 2}, {\color{orange} 4}\}$, 
$\mathcal Y = \{ {\color{ProcessBlue} 1}, {\color{ProcessBlue} 3}, {\color{ProcessBlue} 5}\}$ and define distributions $\color{red}\mu$ and $\color{blue}\nu$ via
\ba
{\color{red} \mu} ({\color{orange} 1}) & = 0.6, 
\ {\color{red} \mu} ({\color{orange} 2}) = 0.3, 
\ {\color{red} \mu} ({\color{orange} 4}) = 0.1 \\
{\color{blue} \nu} ({\color{ProcessBlue} 1}) & = 0.3, 
\ {\color{blue} \nu}({\color{ProcessBlue} 3}) = 0.5, 
\ {\color{blue} \nu}({\color{ProcessBlue} 5}) = 0.2.
\ea
\vspace{-2em}
\begin{center}
\includegraphics[width=0.8\linewidth]{Images/pmf.png}
\end{center}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Example: discrete distribution}
\begin{itemize}
\item[]
The cost matrix ${\color{Green} \mathbf D}_{{\color{orange} i}{\color{ProcessBlue} j}} = {\color{Green} d}({\color{orange} x_i}, {\color{ProcessBlue} y_j})$, using 
${\color{Green} d}({\color{orange} x}, {\color{ProcessBlue} y}) = |{\color{orange} x} - {\color{ProcessBlue} y}|$ as our metric, is
\ba
{\color{Green} \mathbf D} = 
	\begin{blockarray}{cccc}
	& {\color{ProcessBlue} 1} & {\color{ProcessBlue} 3} & {\color{ProcessBlue} 5} \\
	\begin{block}{c(ccc)}
		{\color{orange} 1} & |{\color{orange} 1} - {\color{ProcessBlue} 1}| & |{\color{orange} 1} - {\color{ProcessBlue} 3}| & |{\color{orange} 1} - {\color{ProcessBlue} 5}| \\
		{\color{orange} 2} & |{\color{orange} 2} - {\color{ProcessBlue} 1}| & |{\color{orange} 2} - {\color{ProcessBlue} 3}| & |{\color{orange} 2} - {\color{ProcessBlue} 5}| \\
		{\color{orange} 4} & |{\color{orange} 4} - {\color{ProcessBlue} 1}| & |{\color{orange} 4} - {\color{ProcessBlue} 3}| & |{\color{orange} 4} - {\color{ProcessBlue} 5}| \\
	\end{block}
\end{blockarray}
=
\begin{blockarray}{cccc}
	& {\color{ProcessBlue} 1} & {\color{ProcessBlue} 3} & {\color{ProcessBlue} 5} \\
	\begin{block}{c(ccc)}
		{\color{orange} 1} & \color{Green}0 & \color{Green}2 & \color{Green}4 \\
		{\color{orange} 2} & \color{Green}1 & \color{Green}1 & \color{Green}3 \\
		{\color{orange} 4} & \color{Green}3 & \color{Green}1 & \color{Green}1 \\
	\end{block}
\end{blockarray}
\ea
\vspace{-3em}
\begin{center}
\includegraphics[width=0.7\linewidth]{Images/pmf.png}
\end{center}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Example: "proof" by inspection}
\begin{minipage}{0.49\linewidth}
\begin{itemize}
\item
${\color{blue} \nu}$ needs 0.2 mass at $x = 5$;  the closest mass from ${\color{red} \mu}$ is 0.1 unit at $x = 4$
\item
${\color{blue} \nu}$ still needs 0.1 unit of mass for $x = 5$; get from ${\color{red} \mu}$ at next closest value, $x = 2$
\item
This leaves 0.2 units from ${\color{red} \mu}$ at $x = 2$ which can be moved to $x = 3$ for ${\color{blue} \nu}$
\item
Remaining 0.3 units needed at $x = 3$ for ${\color{blue} \nu}$ can be obtained from $x = 1$ in ${\color{red} \mu}$
\item
This leaves 0.3 units at $x = 1$ for ${\color{blue} \nu}$, as desired.
\end{itemize}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\includegraphics[width=\linewidth]{Images/pmf.png}
\footnotesize
\vspace{-1em}
\ba
{\color{Purple} \mathbf P^*} & = \begin{blockarray}{cccc}
	& {\color{ProcessBlue} 1} & {\color{ProcessBlue} 3} & {\color{ProcessBlue} 5} \\
	\begin{block}{c(ccc)}
		{\color{orange} 1} & \color{Purple} 0.3 & \color{Purple} 0.3 & \color{Purple} 0 \\
		{\color{orange} 2} & \color{Purple} 0 & \color{Purple}  0.2 & \color{Purple} 0.1 \\
		{\color{orange} 4} & \color{Purple} 0 & \color{Purple} 0 & \color{Purple} 0.1 \\
	\end{block}
\end{blockarray} \\
\mathcal L_{{\color{Green} \mathbf D}}(\color{red}\mu, \color{blue}\nu) 
	& = \langle \mathbf {\color{Green} D}, {\color{Purple} \mathbf P^*} \rangle \\
	& = {\color{Green} 2} \cdot {\color{Purple} 0.3} + {\color{Green} 1} \cdot {\color{Purple} 0.2} + {\color{Green} 3} \cdot {\color{Purple} 0.1} + {\color{Green} 1} \cdot {\color{Purple} 0.1} \\
	& = 1.2
\ea
\end{minipage}
\end{frame}

\begin{frame}[fragile]\frametitle{Example: rigorous solution}
\small
To solve for ${\color{Purple} \mathbf P^*}$ rigorously, we can set up a linear program using the row-sums and column-sums constraints:
\bigskip
\ba
\;& \text{minimize} \ {\color{Purple} \mathbf P}_{1,2} + 4{\color{Purple} \mathbf P}_{1,3} + {\color{Purple} \mathbf P}_{2,1} +  {\color{Purple} \mathbf P}_{2, 2} + 3{\color{Purple} \mathbf P}_{2,3} + 3{\color{Purple} \mathbf P}_{3,1} + {\color{Purple} \mathbf P}_{3,2} + {\color{Purple} \mathbf P}_{3, 3} \\
& \text{subject to} \begin{cases}
	 {\color{Purple} \mathbf P}_{1,1} + {\color{Purple} \mathbf P}_{1,2} + {\color{Purple} \mathbf P}_{1,3} = 0.6 \\
	 {\color{Purple} \mathbf P}_{2,1} + {\color{Purple} \mathbf P}_{2,2} + {\color{Purple} \mathbf P}_{2,3} = 0.3 \\
	 {\color{Purple} \mathbf P}_{3,1} + {\color{Purple} \mathbf P}_{3,2} + {\color{Purple} \mathbf P}_{3,3} = 0.1 \\
	 {\color{Purple} \mathbf P}_{1,1} + {\color{Purple} \mathbf P}_{2,1} + {\color{Purple} \mathbf P}_{3,1} = 0.3 \\
	 {\color{Purple} \mathbf P}_{1,2} + {\color{Purple} \mathbf P}_{2,2} + {\color{Purple} \mathbf P}_{3,2} = 0.5 \\
	 {\color{Purple} \mathbf P}_{1,3} + {\color{Purple} \mathbf P}_{2,3} + {\color{Purple} \mathbf P}_{3,3} = 0.2 \\
	 0 \le {\color{Purple} \mathbf P}_{i,j} \le 1 \ \forall i,j 
\end{cases}
\ea
\end{frame}

\begin{frame}\frametitle{Wasserstein distance}
\begin{itemize}
\item[]
$
\mathcal L_{{\color{Green} d}}(
		{\color{red} \mu}, {\color{blue} \nu}) 
	\equiv \text{Wasserstein distance between ${\color{red} \mu}$ and ${\color{blue} \nu}$ w.r.t. ${\color{Green} d}$}
$
\bigskip
\item[]
Discrete case:
\ba
\mathcal L_{{\color{Green} \mathbf D}}(
		{\color{red} \mu}, {\color{blue} \nu})
	& = \min_{{\color{Purple} \mathbf P} \in \Pi({\color{red} \mu}, {\color{blue} \nu})}
		\sum_{i=1}^n \sum_{j=1}^m \mathbf {\color{Green} D}_{i,j} \mathbf {\color{Purple} \mathbf P}_{i,j}
\ea
where
$\mathbf {\color{Green} D}_{i,j} = {\color{Green} d}(x_i, y_j)$ 
and
$\mathbf {\color{Purple} \mathbf P}_{i,j} = {\color{violet} \pi}(x_i, y_j)$
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Admissible couplings}
\begin{itemize}
\item
Write discrete distributions ${\color{red} \mu}, {\color{blue} \nu}$ as vectors ${\color{red} \mathbf r}, {\color{blue} \mathbf c}$ respectively.
\bigskip
\item
Set of admissible couplings:
\ba
\Pi({\color{red} \mathbf r}, {\color{blue} \mathbf c}) 
	= \set{ {\color{Purple} \mathbf P} \in \reals_+^{n \times m} : 
	\mathbf {\color{Purple} \mathbf P} \mathbf{1}_m = {\color{red} \mathbf r} \text{ and }
	\mathbf {\color{Purple} \mathbf P}^\mathsf{T} \mathbf{1}_n = {\color{blue} \mathbf c} }
\ea
\item Then
\begin{align}
\mathcal L_{{\color{Green} \mathbf D}}(
		{\color{red} \mathbf r}, {\color{blue} \mathbf c})
	& = \min_{{\color{Purple} \mathbf P} \in \Pi({\color{red} \mathbf r} , {\color{blue} \mathbf c})}
		\sum_{i=1}^n \sum_{j=1}^m \mathbf {\color{Green} D}_{i,j} \mathbf {\color{Purple} \mathbf P}_{i,j} \\
	& = \min_{{\color{Purple} \mathbf P} \in \Pi({\color{red} \mathbf r} , {\color{blue} \mathbf c})} 
		\langle \mathbf {\color{Green} D}, {\color{Purple} \mathbf P} \rangle 
\end{align}
is a linear program over a convex polytope
\medskip
\item
Complexity: $\mathcal O(k^3 \log k)$, $k = \max(n, m)$ 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Sinkhorn distance}
\begin{itemize}
\item
Cuturi (2013): let's regularize for high entropy of the couplings ${\color{Purple} \mathbf P}$ for simplicity
\bigskip
\item
In particular, they introduce the Sinkhorn distance 
\begin{align}
\mathcal L_{{\color{Green} \mathbf D}}^\lambda(
		{\color{red} \mathbf r}, {\color{blue} \mathbf c})
	:= \left\langle {\color{Green} \mathbf D}, {\color{Purple} \mathbf P}_{{\color{red} \mathbf r}, {\color{blue} \mathbf c}}^\lambda \right\rangle
\end{align}
where
\begin{align}
{\color{Purple} \mathbf P}_{{\color{red} \mathbf r}, {\color{blue} \mathbf c}}^\lambda 
= \argmin_{{\color{Purple} \mathbf P} \in \Pi({\color{red} \mathbf r} , {\color{blue} \mathbf c})}
	\left\{ \langle {\color{Green} \mathbf D}, {\color{Purple} \mathbf P} \rangle - \frac{1}{\lambda} h({\color{Purple} \mathbf P}) \right\}
\end{align}
and $h({\color{Purple} \mathbf P}) 
	:= - \sum_{i=1}^d \sum_{j=1}^d {\color{Purple} \mathbf P}_{i,j} \log({\color{Purple} \mathbf P}_{i,j})$ is the Shannon entropy of ${\color{Purple} \mathbf P}$.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{What is Sinkhorn doing?}
\begin{itemize}
\item
Turns out we're constraining region to a new region $\Pi_\alpha$ with
\begin{align}
\Pi_\alpha({\color{red} \mathbf r} , {\color{blue} \mathbf c})
	= \left\{ {\color{Purple} \mathbf P} \in \Pi({\color{red} \mathbf r} , {\color{blue} \mathbf c}) : 
		\text{KL}\left({\color{Purple} \mathbf P} \ \big{|}\big| \ {\color{red} \mathbf r} {\color{blue} \mathbf c}^{\mathsf T} \right) \le \alpha \right\}.
\end{align}
so that
\ba
\mathcal L_{{\color{Green} \mathbf D}}^\lambda(
		{\color{red} \mathbf r}, {\color{blue} \mathbf c})
	= \min_{{\color{Purple} \mathbf P} \in \Pi_\alpha({\color{red} \mathbf r} , {\color{blue} \mathbf c})} 
		\langle \mathbf {\color{Green} D}, {\color{Purple} \mathbf P} \rangle 
\ea
\begin{center}
\includegraphics[width=0.7\linewidth]{Images/Polytope.png}
\end{center}

\end{itemize}

\end{frame}

\begin{frame}\frametitle{Fast OT via regularization}
\begin{minipage}{0.49\linewidth}
\begin{itemize}
\item
Discrete OT seeks minimum over convex polytope
\medskip
\begin{itemize}
\item
$\mathcal O(d^3 \log d)$, $d = \max(n, m)$
\end{itemize}
\bigskip
\item
Cuturi (2013): regularize space of couplings for high entropy
\medskip
\begin{itemize}
\item
Empirical tests suggest $\mathcal O(d)$ to $\mathcal O(d^2)$
\end{itemize}
\end{itemize}
\end{minipage}
\begin{minipage}{0.49\linewidth}
\begin{center}
\includegraphics[width=\linewidth]{Images/Sinkhorn_diagram.png}
\end{center}
\end{minipage}
\end{frame}


\begin{frame}\frametitle{TCRdist [Dash et al. '17]}
\begin{align}
\text{TCRdist}(r_1, r_2)
	& := \sum_{p \in \text{{\color{ProcessBlue} CDR positions}}} 
		\sum_{(a_1, a_2) \in p}
		{\color{Orange} w}(p) 
		\text{{\color{Green} AAdist}}(a_1, a_2; p)
\end{align}

where:
\footnotesize
\begin{itemize}
\item
$\text{{\color{ProcessBlue} CDR positions}} 
	:= \{\text{CDR1}\alpha, \text{CDR2}\alpha, \text{CDR2.5}\alpha, 
		\text{CDR3}\alpha,
		 \text{CDR1}\beta, \text{CDR2}\beta, \text{CDR2.5}\beta, \text{CDR3}\beta\}$
\item 
$ {\color{Orange} w}(p) := \begin{cases} 3, & p \in \{\text{CDR3}\alpha, \text{CDR3}\beta\} \\
						1, & \text{else}
		\end{cases}$
\item
$ \text{{\color{Green} AAdist}}(a_1, a_2; p) := 
\begin{cases}
	0, & a_1 = a_2 \\
	8, & a_1 = \text{`}\texttt{-}\text{'} \oplus a_2 = \text{`}\texttt{-}\text{'}, 
		\ p \in \{\text{CDR3}\alpha, \text{CDR3}\beta\} \\
	4, & a_1 = \text{`}\texttt{-}\text{'} \oplus a_2 = \text{`}\texttt{-}\text{'}, 
		\ p \not\in \{\text{CDR3}\alpha, \text{CDR3}\beta\} \\

	4 - \min(0,  \text{{\color{Purple} BLOSUM62}}(a_1, a_2)), & \text{else} 
\end{cases}$ 
\item
{\color{Purple} BLOSUM62}: widely-used AA substitution matrix
[Henikoff '92]
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Initial results}
\begin{itemize}
\item
Are these loneliness scores biologically plausible?
\bigskip
\item
Consider 2 groups of biological replicates: group of CD4+ mice, and group of DN mice
\bigskip
\item
Randomly choose a DN representative, $D$
\medskip
\item
Compute score distributions for each TCR $\in D$ to each CD4+ repertoire (foreground), and every other DN repertoire (background)
\bigskip
\item
Hope: foreground scores tend to be larger than background scores 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Score ECDFs}
\begin{center}
\includegraphics[width=\linewidth]{Images/mean_ecdfs.pdf}
\end{center}
\end{frame}


\end{document}























